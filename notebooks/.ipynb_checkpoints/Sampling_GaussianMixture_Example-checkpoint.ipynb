{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../widgets/config_check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "import numpy as np\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "from scipy.special import logsumexp\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load classes and functions from the previous parts\n",
    "from jupyter_cms.loader import load_notebook\n",
    "smpl_intro = load_notebook('./Sampling_Intro.ipynb')\n",
    "smpl_rej = load_notebook('./Sampling_Rejection.ipynb')\n",
    "smpl_index = load_notebook('./Sampling_Index.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_index.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Example:** Gibbs sampling for Gaussian mixture model\n",
    "\n",
    "Many machine learning applications require that we can model probability densities, including ones with complicated shapes and in high dimensions. Parametric models, such as Gaussian distributions, are well studied and understood, but only provide good models if its assumptions of unimodality and rather light tails are matched by the data. Mixture models provide a principled and flexible way to extend the type of densities that can be modeled. Here, the density is assumed to be composed of different (mixture) components each of which can be described by a simple parametric form. Especially Gaussian mixture models, not least due to efficient estimation algorithms, are vastly popular in machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian mixture model describes a density $p(\\mathbf{x})$ as follows:\n",
    "$$ p(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_k, \\mathbf{Sigma}_k) $$\n",
    "Thus, it models the density as a sum/mixture of $K$ Gaussian components. Here, $(\\pi_k)_{k=1}^K$ denotes the probability of each component and $\\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k$ are the component means and covariances respectively.\n",
    "\n",
    "Introducing a latent variable $z \\in \\{1,\\ldots,K\\}$ we can also think of a datum $\\mathbf{x}$ as being generated by the following process:\n",
    "$$ \\begin{array}{clc} z & \\sim & \\pi \\\\ \\mathbf{x} & \\sim & \\mathcal{N}(\\mathbf{\\mu}_z, \\mathbf{\\Sigma}_z) \\end{array} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, to estimate parameters of a Gaussian mixture model from a data set $\\mathcal{X} = (\\mathbf{x}_n)_{n=1}^N$, we need to\n",
    "\n",
    "* infer the hidden component assignments $(z_n)_{n=1}^K$\n",
    "* infer the component means $(\\mathbf{\\mu}_k)_{k=1}^K$ and covariances $(\\mathbf{\\Sigma}_k)_{k=1}^K$\n",
    "\n",
    "As a Bayesian, we do this by computing the corresponding posterior distributions. Unfortunately, since the assignments $(z_n)$ are unobserved, the likelihood needs to marginalize over all possible component assignments leading to an intractable sum over exponentially many terms. Thus, in practice we need to approximate the posterior, and many such algorithms have been developed for the Gaussian mixture model:\n",
    "\n",
    "* **EM algorithm**: The *expectation-maximization* algorithm computes the maximum likelihood parameters of the mixture components\n",
    "* **Variational Bayes**: Similar to the EM algorithm, but computing an approximate posterior distribution over the parameters\n",
    "* **Gibbs sampling**: Sampling algorithm which explores the posterior distribution, over assignments and parameters.\n",
    "\n",
    "Here, we will implement Gibbs sampling. Thus, we need to be able to compute conditional posterior distributions. In the Gaussian mixture model this requires that we use conjugate priors leading to the following generative model:\n",
    "$$ \\begin{array}{lcl} \\pi & \\sim & \\mathcal{D}irichlet(\\mathbf{\\alpha}) \\\\\n",
    "  \\mathbf{\\mu}_, \\mathbf{\\Lambda}_k & \\sim & \\mathcal{W}ishart-\\mathcal{N}ormal(\\mathbf{\\mu}_0, \\nu, \\kappa, \\mathbf{S}_0) \\quad \\forall k = 1,\\ldots,N \\\\\n",
    "  z_n & \\sim & \\pi \\\\\n",
    "  \\mathbf{x}_n & \\sim & \\mathcal{N}(\\mathbf{\\mu}_{z_n}, \\mathbf{\\Lambda}^{-1}_{z_n}) \\quad \\forall n = 1,\\ldots,N \n",
    "  \\end{array} $$\n",
    "\n",
    "**Exercise:** Show that the Dirichlet distribution is the conjugate prior for the categorical variable $Z$.\n",
    "\n",
    "The conjugate prior for a multi-variate Gaussian with unknown mean and precision matrix is the *Normal-Wishart* distribution. For the math, e.g. posterior parameters, we refer to K. Murphy, *Conjugate Bayesian analysis of the Gaussian distribution*. Here, we use these results to implement the *collapsed Gibbs sampler*, i.e. with the mixture parameters $\\mathbf{\\mu}_k, \\mathbf{\\Lambda}_k$ integrated out analytically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we approximate the marginal posterior $p(z_1,\\ldots,z_N | \\mathcal{X})$ by sampling from its conditional distributions:\n",
    "$$ \\begin{array}{lcl} p(z_i | \\mathcal{X}, \\mathbf{z}_{-i}) & \\propto & p(z_i | \\mathbf{z}_{-i}) p(\\mathcal{X} | z_i, \\mathbf{z}_{-i}) \\\\\n",
    "& = & p(z_i | \\mathbf{z}_{-i}) p(\\mathbf{x}_i | \\mathcal{X}_{-i}, z_i, \\mathbf{z}_{-i}) p(\\mathcal{X}_{-i} | z_i, \\mathbf{z}_{-i}) \\\\\n",
    "& \\propto & p(z_i | \\mathbf{z}_{-i}) p(x_i | \\mathcal{X}_{-i}, z_i, \\mathbf{z}_{-i}) \\end{array}$$\n",
    "where $_{-i}$ denotes a vector without the $i$th component and we have used conditional independence in the model, namely that $p(\\mathcal{X}_{-i} | z_i, \\mathbf{z}_{-i}) = p(\\mathcal{X}_{-i} | \\mathbf{z}_{-i})$.\n",
    "\n",
    "Thanks to conjugacy both terms can be computed analytically, being the predictive distributions from the Dirichlet and Normal-Wishart posteriors. For the second term note that $p(\\mathbf{x}_i | \\mathcal{X}_{-i}, z_i, \\mathbf{z}_{-i})$ only depends on the data points that are assigned to the same component as $z_i$. In order to compute the corresponding distributions for each component, we keep track of the (unnormalized) empirical means, i.e. $\\sum_{n=1, z_n = k}^K \\mathbf{x}_n$, and covariance terms, i.e. $\\sum_{n=1, z_n = k}^K \\mathbf{x}_n \\mathbf{x}_n^T$.\n",
    "\n",
    "A step of the collapsed Gibbs sampler then works as follows:\n",
    "\n",
    "* For $i = 1:N$\n",
    "  1. Remove $\\mathbf{x}_i$ from component $z_i$\n",
    "  2. For $k = 1:K$\n",
    "       \n",
    "       Compute $p(z_i=k | \\mathcal{X}, \\mathbf{z}_{-i}) \\propto p(z_i=k | \\mathbf{z}_{-i}) p(x_i | \\mathcal{X}_{-i}, z_i=k, \\mathbf{z}_{-i})$\n",
    "       \n",
    "  3. Draw new assignment $z'_i$ from this distribution (after normalization) \n",
    "  4. and add $\\mathbf{x}_i$ to component $z'_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "class GaussStatistics (object):\n",
    "    \"\"\"\n",
    "    Stores statistics of data for Gaussian model\n",
    "    \"\"\"\n",
    "    def __init__(self, X):\n",
    "        N, D = X.shape\n",
    "        # Store number of samples\n",
    "        self.N = N\n",
    "        # sum of samples\n",
    "        self.x = np.reshape(np.sum(X, axis=0), (D,1))\n",
    "        # and sum of outer squares\n",
    "        self.xxT = np.dot(X.T, X)\n",
    "\n",
    "# Computations for conjugate priors\n",
    "def posterior_NormalWishart (stats, mu0, nu, kappa, S0):\n",
    "    \"\"\"\n",
    "    Computes posterior parameters of Normal-Wishart\n",
    "\n",
    "    see Murphy, page 18\n",
    "    \"\"\"\n",
    "    nuN = nu + stats.N\n",
    "    kappaN = kappa + stats.N\n",
    "    # Note: Formula adapted to statistics\n",
    "    muN = (kappa*mu0 + stats.x)/kappaN\n",
    "    SN = S0 + stats.xxT + kappa*np.dot(mu0, mu0.T) - kappaN*np.dot(muN, muN.T)\n",
    "    return muN, nuN, kappaN, SN\n",
    "\n",
    "def predictive_NormalWishart (x, muN, nuN, kappaN, SN):\n",
    "    \"\"\"\n",
    "    Predictive distribution of Normal-Wishart\n",
    "\n",
    "    see Murphy, page 19\n",
    "    \"\"\"\n",
    "    D, k = x.shape\n",
    "    return mvt_log_pdf(x, muN, nuN-D+1, SN*(kappaN+1)/(kappaN*(nuN-D+1)))\n",
    "\n",
    "def mvt_log_pdf (x, mu, nu, Sigma):\n",
    "    \"\"\"\n",
    "    Compute log density of multi-variate Student t distribution\n",
    "    \"\"\"\n",
    "    D, k = x.shape\n",
    "    return np.math.lgamma((nu + D)/2.) - np.math.lgamma(nu/2.) - D/2.*np.log(nu*np.pi) \\\n",
    "        - 1/2.*np.linalg.slogdet(Sigma)[1] \\\n",
    "        - ((nu+D)/2.)*np.log(1 + 1/nu*np.dot((x - mu).T, np.linalg.solve(Sigma, x - mu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "# Finally, we can implement the collapsed Gibbs sampler\n",
    "def gmm_Gibbs_step (X, z, comp_stats, alpha, mu0, nu, kappa, S0):\n",
    "    \"\"\"\n",
    "    Run single Gibbs step, i.e. update each assignment z_i\n",
    "    \n",
    "    comp_stats contains GaussianStatistics for each mixture component\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    assert len(z) == N, 'each data point must be assigned to exactly one mixture component'\n",
    "    K = len(comp_stats)\n",
    "    assert np.min(z) >= 0 and np.max(z) < K, 'assignment and components do not match'\n",
    "\n",
    "    for i in range(len(z)):\n",
    "        # Current assignment\n",
    "        zi = z[i]\n",
    "\n",
    "        # Fetch data point and remove it from statistics\n",
    "        xi = np.reshape(X[i,:], (D,1))\n",
    "        Ni = comp_stats[zi].N\n",
    "        comp_stats[zi].N = Ni - 1\n",
    "        comp_stats[zi].x = comp_stats[zi].x - xi\n",
    "        comp_stats[zi].xxT = comp_stats[zi].xxT - np.dot(xi, xi.T)\n",
    "        \n",
    "        # Compute conditional probabilities for zi\n",
    "        log_cond_probs = np.zeros(K)\n",
    "        for k in range(K):\n",
    "            post_i = posterior_NormalWishart (comp_stats[k], mu0, nu, kappa, S0)\n",
    "            log_pred_xi = predictive_NormalWishart(xi, *post_i)\n",
    "            # Predictive cond. probability from Dirichlet\n",
    "            log_pred_diri = np.log((comp_stats[k].N + alpha)/(N + K*alpha - 1)) # - 1 ???\n",
    "            log_cond_probs[k] = log_pred_diri + log_pred_xi\n",
    "\n",
    "            # if i == 0:\n",
    "            #     print comp_stats[k].N, log_pred_diri, log_pred_xi\n",
    "\n",
    "        # Normalize probs in log space\n",
    "        log_cond_probs -= logsumexp(log_cond_probs)\n",
    "        \n",
    "        # Sample new assignment from this distribution\n",
    "        zi_new = np.random.choice(K, p=np.exp(log_cond_probs))\n",
    "        z[i] = zi_new\n",
    "        # and add xi to its statistics\n",
    "        Ni_new = comp_stats[zi_new].N\n",
    "        comp_stats[zi_new].N = Ni_new + 1\n",
    "        comp_stats[zi_new].x = comp_stats[zi_new].x + xi\n",
    "        comp_stats[zi_new].xxT = comp_stats[zi_new].xxT + np.dot(xi, xi.T)\n",
    "\n",
    "    # Finally, return new assignments and statistics\n",
    "    return z, comp_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test the whole thing on some demo data and with nice graphics $\\ldots$\n",
    "\n",
    "Note that there are different choices of how the initial assignment can be chosen:\n",
    "\n",
    "* All in one component, i.e. $z_n = 1$ for all $n$\n",
    "* At random, i.e. each data point is assigned to a random component $1, \\ldots, K$\n",
    "* Using some clustering algorithm, e.g. K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "def gmm_Gibbs_demo (X, K, alpha, mu0, nu, kappa, S0, iter=100):\n",
    "    \"\"\"\n",
    "    Run Gibbs sampler on Gaussian mixture model with K components and\n",
    "    the given hyperparameters\n",
    "\n",
    "    Plot fancy stuff on each iteration\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    assert D == 2, 'cannot plot data with more than 2 dimensions'\n",
    "    # Draw initial assignment at random\n",
    "    z = np.random.choice(K, size=N)\n",
    "    z = np.array([0 for _ in range(N)])\n",
    "    # z = np.concatenate(([0 for _ in range(100)], [1 for _ in range(100)], [2 for _ in range(30)]))\n",
    "    # and initialize component statistics\n",
    "    comp_stats = [GaussStatistics(X[z == k,:]) for k in range(K)]\n",
    "\n",
    "    fig = plt.figure(1)\n",
    "    # Plot 1-stdev ellipse for each mixture component\n",
    "    for k in range(K):\n",
    "        muN, nuN, kappaN, SN = posterior_NormalWishart (comp_stats[k], mu0, nu, kappa, S0)\n",
    "        plot_cov_ellipse(SN/nuN, muN, nstd=1, alpha=0.1)\n",
    "        plt.scatter(muN[0], muN[1], marker='x', s=50)\n",
    "    plt.scatter(X[:,0], X[:,1], c=z)\n",
    "\n",
    "    # Plot predictive density\n",
    "    def pred (x, comp_stats, post_K):\n",
    "        K = len(comp_stats)\n",
    "        return np.exp(logsumexp([np.log((comp_stats[k].N + alpha)/(N + alpha*K)) + predictive_NormalWishart(x, *post_K[k]) \\\n",
    "                                 for k in range(K)]))\n",
    "    xmin, xmax, ymin, ymax = plt.axis()\n",
    "    fun_contour(np.linspace(xmin, xmax, 50), np.linspace(ymin, ymax, 50), \\\n",
    "                lambda x,y: pred(np.array([[x],[y]]), comp_stats, \\\n",
    "                                 [posterior_NormalWishart(comp_stats[k], mu0, nu, kappa, S0) for k in range(K)]), \\\n",
    "                cmap='gray',\n",
    "                alpha=0.2)\n",
    "        \n",
    "    plt.pause(0.1)\n",
    "    \n",
    "    for i in range(iter):\n",
    "        z, comp_stats = gmm_Gibbs_step (X, z, comp_stats, alpha, mu0, nu, kappa, S0)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print (i)\n",
    "            fig.clear()\n",
    "            for k in range(K):\n",
    "                muN, nuN, kappaN, SN = posterior_NormalWishart (comp_stats[k], mu0, nu, kappa, S0)\n",
    "                plot_cov_ellipse(SN/nuN, muN, nstd=1, alpha=0.1)\n",
    "                # Plot large mark for posterior mean\n",
    "                plt.scatter(muN[0], muN[1], marker='x', s=50)\n",
    "            fun_contour(np.linspace(xmin, xmax, 50), np.linspace(ymin, ymax, 50), \\\n",
    "                        lambda x,y: pred(np.array([[x],[y]]), comp_stats, \\\n",
    "                                         [posterior_NormalWishart(comp_stats[k], mu0, nu, kappa, S0) for k in range(K)]), \\\n",
    "                        cmap='gray',\n",
    "                        alpha=0.2)\n",
    "\n",
    "            plt.scatter(X[:,0], X[:,1], c=z)\n",
    "            fig.canvas.draw()\n",
    "\n",
    "    return z, comp_stats\n",
    "\n",
    "# Helper function from: https://github.com/joferkington/oost_paper_code/blob/master/error_ellipse.py\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def plot_cov_ellipse(cov, pos, nstd=2, ax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Plots an `nstd` sigma error ellipse based on the specified covariance\n",
    "    matrix (`cov`). Additional keyword arguments are passed on to the \n",
    "    ellipse patch artist.\n",
    "    Parameters\n",
    "    ----------\n",
    "        cov : The 2x2 covariance matrix to base the ellipse on\n",
    "        pos : The location of the center of the ellipse. Expects a 2-element\n",
    "            sequence of [x0, y0].\n",
    "        nstd : The radius of the ellipse in numbers of standard deviations.\n",
    "            Defaults to 2 standard deviations.\n",
    "        ax : The axis that the ellipse will be plotted on. Defaults to the \n",
    "            current axis.\n",
    "        Additional keyword arguments are pass on to the ellipse patch.\n",
    "    Returns\n",
    "    -------\n",
    "        A matplotlib ellipse artist\n",
    "    \"\"\"\n",
    "    def eigsorted(cov):\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        return vals[order], vecs[:,order]\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "\n",
    "    # Width and height are \"full\" widths, not radius\n",
    "    width, height = 2 * nstd * np.sqrt(vals)\n",
    "    ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, **kwargs)\n",
    "\n",
    "    ax.add_artist(ellip)\n",
    "    return ellip\n",
    "\n",
    "# Helper function to plot nice, smooth contours of a function\n",
    "def fun_contour (xr, yr, f, **im_kargs):\n",
    "    \"\"\"\n",
    "    Evaluates function over a regular grid xr x yr and plots smooth,\n",
    "    colored contours on current axes\n",
    "    \"\"\"\n",
    "    xg, yg = np.meshgrid(xr, yr)\n",
    "    zg = np.reshape([f(x,y) for x,y in zip(xg.ravel(), yg.ravel())], xg.shape)\n",
    "    plt.imshow(zg, interpolation='bilinear', vmin=zg.min(), vmax=zg.max(), \\\n",
    "               extent = [xg.min(), xg.max(), yg.min(), yg.max()], \\\n",
    "               origin='lower', **im_kargs)\n",
    "    #plt.contour(xg, yg, zg, 42, **im_kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the whole thing on some demo data\n",
    "X_mix = np.concatenate((np.random.multivariate_normal(np.array([0, 0]), np.array([[1, 0], [0, 1]]), size=100), \\\n",
    "                        np.random.multivariate_normal(np.array([-3, 3]), np.array([[0.5, 0.2], [0.2, 0.5]]), size=70), \\\n",
    "                        np.random.multivariate_normal(np.array([5, 5]), np.array([[1, -0.8], [-0.8, 1]]), size=30)), \\\n",
    "                       axis = 0)\n",
    "\n",
    "N, D = X_mix.shape\n",
    "K = 5\n",
    "gmm_Gibbs_demo(X_mix, K, 1, np.zeros((2,1)), D, 1, np.eye(D));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Explore the collapsed Gibbs sampler on different data sets, e.g. with two components of varying size and separation. When does it work well and when does it fail to mix?\n",
    "\n",
    "**Exercise**: Implement the uncollapsed Gibbs sampler, i.e. sampling from the full posterior\n",
    "$$ p(z_1,\\ldots,z_N, \\{\\mathbf{\\mu}\\}_{k=1}^N, \\{\\mathbf{\\Lambda}\\}_{k=1}^N | \\mathcal{X}) $$\n",
    "Hint: Use conjugate priors, i.e. $\\mu_k \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ and $\\Lambda_k \\sim \\mathcal{W}ishart(\\nu, \\mathbf{S}_0)$, and exploit conditional independencies, i.e. $$p(x_i | \\mathcal{X}_{-i}, (z_n)_{n=1}^N, \\{\\mathbf{\\mu}\\}_{k=1}^N, \\{\\mathbf{\\Lambda}\\}_{k=1}^N) = p(x_i | \\mathbf{\\mu}_{z_i}, \\mathbf{\\Lambda}_{z_i}) $$"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "efc7a14bd599495583068b7ee3d22552",
   "lastKernelId": "12c64970-8927-46b6-bdff-6552f78426d6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
