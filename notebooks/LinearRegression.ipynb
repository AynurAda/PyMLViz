{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../widgets/config_check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The interactive plots are created by using the libraries bqplot and ipywidgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from jupyter_cms.loader import load_notebook\n",
    "lrw = load_notebook('../widgets/LinRegWidget.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is a standard tool in statistics and machine learning expressing a linear relationship between several independent $\\mathbf{x} = x_1, \\ldots, x_D$ and a dependent variable $t$ (target):\n",
    "$$ t = w_0 + \\sum_{i = 1}^D w_i x_i + \\epsilon $$\n",
    "where $\\epsilon$ denotes the residual error of the model. The *regression weights* $w_0, w_1, \\ldots, w_D$ are parameters of the model linking inputs $\\mathbf{x}$ with targets $t$. In the following, we will denote the model predictions as $y = w_0 + \\sum_{i = 1}^D w_i x_i$. These can be considered as noise less versions of the target outputs and capture the structure in the data that is modeled by linear regression.\n",
    "\n",
    "In general, machine learning distinguishes between *supervised* and *unsupervised learning*.\n",
    "\n",
    "* Supervised: Target outputs are known and can be used for training (labelled data)\n",
    "  * Regression: Continuous target outputs\n",
    "  * Classification: Continuous target outputs\n",
    "  \n",
    "  Examples: Time-series prediction, face recognition\n",
    "\n",
    "* Unsupervised: Target outputs are either not defined or unavailable.\n",
    "\n",
    "  Examples: Density estimation, clustering\n",
    "\n",
    "In the terminology of machine learning, linear regression is an example of supervised learning with continuous targets. \n",
    "Thus, it assumes that a labelled data set $\\mathbf{D} = \\{(\\mathbf{x}_n, t_n)\\}_{n = 1}^N$ containing $N$ training examples of input/target pairs is available. To find the linear regression weights, we need a way to measure the performance of models with different weights. A common choice is the *squared error* between model predictions and actual target outputs, i.e.\n",
    "$$ \\mathtt{MSE}_{\\mathbf{D}}(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N (w_0 + \\sum_i w_i x_{ni} - t_n)^2 $$\n",
    "\n",
    "The optimal regression weights are then found by minimizing the squared error, i.e.\n",
    "$$ w_{LR} = \\mathtt{argmin}_{\\mathbf{w}} \\mathtt{MSE}_{\\mathbf{D}}(\\mathbf{w}) = \\mathtt{argmin}_{\\mathbf{w}}\\frac{1}{2} \\sum_{n=1}^N (w_0 + \\sum_i w_i x_{ni} - t_n)^2 $$\n",
    "\n",
    "Linear regression can be conveniently expressed in matrix notation:\n",
    "$$ t = \\mathbf{w}^T \\mathbf{x} + \\mathbf{\\epsilon} $$\n",
    "where $\\mathbf{w}^T$ denotes the transpose of the weight vector and $\\mathbf{x}$ is assumed to be extended with $x_0 \\equiv 1$. By this trick the model predictions $y = \\mathbf{w}^T \\mathbf{x}$ are simply given by inner product between the weight and input vectors.\n",
    "\n",
    "Correspondingly, the above minimization problem can be written as\n",
    "$$ w_{LR} = \\mathtt{argmin}_{\\mathbf{w}} \\frac{1}{2} \\sum_{n=1}^N (t_n - \\mathbf{w}^T \\mathbf{x}_n)^2 $$\n",
    "The minimization problem can be solved analytically by taking the derivative and setting it to zero:\n",
    "\\begin{eqnarray*}\n",
    "  \\frac{\\partial}{\\partial w_i} \\frac{1}{2} \\sum_{n=1}^N (t_n - \\mathbf{w}^T \\mathbf{x}_n)^2 & = & \n",
    "  \\sum_{n=1}^N (t_n - \\mathbf{w}^T \\mathbf{x}_n) x_{in} \\overset{!}{=} 0  \\quad \\forall i \\\\\n",
    "  \\implies {\\mathbf{w}_{LR}}^T \\left( \\sum_{n=1}^N \\mathbf{x}_n \\mathbf{x}_n^T \\right) & = & \\sum_{i=1}^N t_n       \\mathbf{x}_n^T\n",
    "\\end{eqnarray*}\n",
    "Collecting all training samples into the design matrix $\\mathbf{X} = (\\mathbf{x}_1^T, \\ldots, \\mathbf{x}_N^T) \\in \\mathbb{R}^{N \\times (D+1)}$ and target vector $\\mathbf{t} = (t_1, \\ldots, t_N)^T$, the optimal regression weights can be written as\n",
    "$$ \\mathbf{w}_{LR} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{t} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly following the API of sklearn, linear regression can be implemented in Python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Simple linear regression\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Create a linear regression model\n",
    "        t = X w + noise\n",
    "        \n",
    "        that minimizes\n",
    "        ||X w - t||_2\n",
    "        \"\"\"\n",
    "        self.weights_ = None\n",
    "        \n",
    "    def fit(self, X, t):\n",
    "        \"\"\"\n",
    "        Fit linear model on training data D = (X, t)\n",
    "        \"\"\"\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights_ = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, t))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict model response on inputs X\n",
    "        \"\"\"\n",
    "        num_samples, num_features = X.shape\n",
    "        \n",
    "        return np.dot(X, self.weights_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that instead of computing the matrix $(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T$, also called *pseudo-inverse* of $\\mathbf{X}$, we solve the linear system of equations $\\mathbf{w}_{LR}^T (\\mathbf{X}^T \\mathbf{X}) = \\mathbf{X}^T \\mathbf{t}$ as matrix inversion is numerically rather unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test our linear regression class on a simple example data set ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 25\n",
    "x = np.random.uniform(size = N)\n",
    "t = 2 * x - 1 + np.random.normal(loc = 0, scale = 0.3, size = N)\n",
    "## Create design matrix with x_0 = 1\n",
    "X = np.hstack((np.ones((N,1)), x[:, None]))\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, t)\n",
    "print(lm.weights_)\n",
    "## Show training data and model predictions\n",
    "x_pred = np.linspace(-0.5, 1.5, 251)\n",
    "X_pred = np.hstack((np.ones((len(x_pred), 1)), x_pred[:, None]))\n",
    "y_pred = lm.predict(X_pred)\n",
    "\n",
    "plt.plot(x, t, 'r.');\n",
    "plt.plot(x_pred, y_pred, 'b-');\n",
    "plt.xlabel('x', fontsize=15)\n",
    "plt.ylabel('t', fontsize=15)\n",
    "plt.title(\"Linear regression example\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression can easily be generalized to model non-linear relationships using *basis functions* $\\Phi_i(\\mathbf{x}), i = 0, \\ldots, M$:\n",
    "$$ y = \\sum_{i=0}^M \\Phi_i(\\mathbf{x}) + \\epsilon $$\n",
    "Common choices of basis functions include\n",
    "* Polynomials, i.e. $\\Phi_i(x) = x^i$\n",
    "* Radial basis functions, i.e. $\\Phi_i(x) = e^{- \\frac{1}{2} \\frac{(x -\n",
    "        \\mu_i)^2}{\\sigma_i^2}}$\n",
    "        \n",
    "As before the intercept is conviently included by choosing $\\Phi_0(\\mathbf{x}) \\equiv 1$.\n",
    "\n",
    "Note that the dimension of the model, i.e. the number of weight parameters, is now determined by the number of basis functions $M$ instead of the input dimension $\\mathbf{x} \\in \\mathbb{R}^D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, roughly following the API of sklearn linear regression with basis functions can be implemented as a preprocessing pipeline, i.e. the data are first changed -- via some basis functions -- before they are passed on to an actual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterceptFeature:\n",
    "    \"\"\"\n",
    "    Constant intercept feature\n",
    "    \"\"\"\n",
    "    def transform(self, x):\n",
    "        return np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "\n",
    "class PolynomialFeatures:\n",
    "    \"\"\"\n",
    "    Polynomial features\n",
    "    \"\"\"\n",
    "    def __init__(self, degree):\n",
    "        self.__degree = degree\n",
    "    \n",
    "    def transform(self, x):\n",
    "        \"\"\"\n",
    "        Compute features x**i for i = 0, ..., degree\n",
    "        \"\"\"\n",
    "        return np.hstack([x**i for i in range(self.__degree + 1)])\n",
    "    \n",
    "class GaussianBasisFunctions():\n",
    "    \"\"\"\n",
    "    Transform the input with a gaussian function of the form:\n",
    "    phi(x) = exp(- kernelsize(=500) * (x - mu) ** 2)\n",
    "    \"\"\"\n",
    "    def __init__(self, mus, sigma = 1.0):\n",
    "        self.mus = mus\n",
    "        self.sigma = sigma\n",
    "      \n",
    "    def transform(self, X):\n",
    "        phi = np.repeat(X,np.shape(self.mus)[0], axis = 1)\n",
    "        phi_trans = phi.T - self.mus\n",
    "        phi_trans = np.exp(- 0.5 / self.sigma**2 * phi_trans.T**2)\n",
    "        return phi_trans    \n",
    "    \n",
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    Model pipeline of preprocessing steps and actual model\n",
    "    \"\"\"\n",
    "    def __init__(self, steps):\n",
    "        self.steps = steps\n",
    "    def fit(self, X, y):\n",
    "        for step in self.steps[:-1]:\n",
    "            X = step.transform(X)\n",
    "        ## Last step of pipeline is actual model\n",
    "        self.steps[-1].fit(X, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        for step in self.steps[:-1]:\n",
    "            X = step.transform(X)\n",
    "        return self.steps[-1].predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These basis functions then transform the original input data $\\mathbf{x}$ and the linear regression is based on the values $\\Phi_1(\\mathbf{x}), \\ldots, \\Phi_M(\\mathbf{x})$ instead. Below an example with $x \\in \\mathbb{R}$ is shown:\n",
    "* Polynomial basis functions $\\Phi_i(x) = x^i$ for $i = 0, 1, 2, 3$\n",
    "* Radial basis functions $\\Phi_i(x) = e^{- \\frac{1}{2} \\frac{(x - \\mu_i)^2}{\\sigma^2}}$ with $\\mu_1 = -1, \\mu_2 = 0, \\mu_3 = 1$ and width $\\sigma = 0.4$\n",
    "The regression for the point $x = 0.25$ is then based on the output of the corresponding basis functions (marked with a dot in the figure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_pred, PolynomialFeatures(3).transform(x_pred[:, None]), '-');\n",
    "xp = np.array([[0.25]])\n",
    "plt.plot(xp, PolynomialFeatures(3).transform(xp), 'k.')\n",
    "plt.xlabel('x', fontsize=15)\n",
    "plt.ylabel(r'$\\Phi_i(x) = x^i$', fontsize=15)\n",
    "plt.legend([r'$x^0$', r'$x^1$', r'$x^2$', r'$x^3$'], fontsize=10)\n",
    "plt.title(\"Polynomial basis functions up to degree 3\", fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = np.array([-1, 0, 1])[:, None]\n",
    "plt.plot(x_pred, GaussianBasisFunctions(mus = mus, sigma = 0.4).transform(x_pred[:, None]), '-');\n",
    "plt.plot(xp, GaussianBasisFunctions(mus = mus, sigma = 0.4).transform(xp), 'k.');\n",
    "plt.xlabel('x', fontsize=15)\n",
    "plt.ylabel(r'$\\Phi_i(x) = e^{- \\frac{1}{2} \\frac{(x - \\mu_i)^2}{\\sigma^2}}$', fontsize=15)\n",
    "plt.legend([r'$\\sigma=0.4,\\, \\mu=-1$', r'$\\sigma=0.4,\\, \\mu=0$', r'$\\sigma=0.4,\\, \\mu=1$'], fontsize=10)\n",
    "plt.title(\"Radial basis functions of width 0.4 centered at -1, 0, 1\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using 8 radial basis functions equally spaced between -1 and 1, a non-linear regression line can be computed for the original example data.\n",
    "As we know that the data where generated with a true linear relationship we can note that the model now starts to fit the noise in the data. This is known as *over-fitting* and leads to wrong predictions for unseen input points. Especially when extrapolating beyond the range training data, the model can fail miserably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm = Pipeline([GaussianBasisFunctions(np.linspace(-1, 1, 8)[:, None], sigma = 0.4),\n",
    "                InterceptFeature(), ## add constant basis function\n",
    "                LinearRegression()])\n",
    "nlm.fit(x[:, None], t)\n",
    "## Show training data and model predictions\n",
    "y_pred = nlm.predict(x_pred[:, None])\n",
    "\n",
    "plt.plot(x, t, 'r.');\n",
    "plt.plot(x_pred, y_pred, 'b-');\n",
    "plt.ylim(-3, 3);\n",
    "plt.xlabel('x', fontsize=15)\n",
    "plt.ylabel('t', fontsize=15)\n",
    "plt.title(\"Non-linear regression example\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feeling for the role of different basis functions, we create several example data sets and an interactive plot  that allows to choose different options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = lrw.LinRegWidget(L2=False)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with flexible models, i.e. polynomial of high degree or radial basis functions with small lengthscale, the fit is very sensitive to single data points. In order to prevent this over-fitting and enable reasonable fits even with flexible models, we need to control the complexity of the model.\n",
    "\n",
    "### Regularization\n",
    "\n",
    "A simple and widely used technique to control model complexity is regularization. Here, instead of minimizing the squared error between model predictions and data points, a penalized error function is used:\n",
    "$$ w_{L2} = \\mathtt{argmin}_{\\mathbf{w}} \\frac{1}{2} \\sum_{n=1}^N (t_n - \\mathbf{w}^T \\mathbf{x}_n)^2 + \\frac{\\lambda}{2} || \\mathbf{w} ||_2 $$\n",
    "Thus, large weights are disfavored by penalizing the L2 norm of the weight vector, i.e. $|| \\mathbf{w} ||_2 = \\sum_i w_i^2$.\n",
    "\n",
    "Exercise: Compute the optimal weight vector $\\mathbf{w}_{L2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionL2:\n",
    "    \"\"\"\n",
    "    Linear regression with L2 regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lam):\n",
    "        \"\"\"\n",
    "        Create a linear regression model\n",
    "        t = X w\n",
    "        \n",
    "        that minimizes\n",
    "        ||X w - t||_2 + \\\\lambda || w ||_2\n",
    "        \"\"\"\n",
    "        self.weights_ = None\n",
    "        self.lambda_ = lam\n",
    "        \n",
    "    def fit(self, X, t):\n",
    "        \"\"\"\n",
    "        Fit linear model on training data D = (X, t)\n",
    "        \"\"\"\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights_ = np.linalg.solve(np.dot(X.T, X) + self.lambda_ * np.identity(num_features),\n",
    "                                        np.dot(X.T, t))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict model response on inputs X\n",
    "        \"\"\"\n",
    "        num_samples, num_features = X.shape\n",
    "        \n",
    "        return np.dot(X, self.weights_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotL2 = lrw.LinRegWidget(L2=True)\n",
    "plotL2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using a regularized fit we can fit much more flexible models without being overly sensitive to single data points. A theoretical explanations of why this is the case can be given in terms of the *bias-variance decomposition*. Here, instead we consider Bayesian linear regression as an alternative, principled way to control model complexity."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
