{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../widgets/config_check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "from __future__ import division\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load classes and functions from the previous parts\n",
    "from jupyter_cms.loader import load_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling methods allow to draw random samples from a given probability distribution. While special methods efficiently draw samples from common distribution, e.g. Gaussian, Poisson, ..., more general methods can draw samples from complicated, high-dimensional distributions. Common applications include:\n",
    "\n",
    "* Random number generators in standard libraries employ sampling methods\n",
    "* Bayesian models can often not be solved exactly as the normalization constant of the posterior cannot be computed in closed form and approximation or sampling methods are needed.\n",
    "\n",
    "Samples are generally useful, as they can be used to approximate expectations. Consider the expectation of $f(x)$ under some distribution with density $p(x)$, i.e.\n",
    "$$ \\mathbb{E}_p[f] = \\int f(x) p(x) dx $$\n",
    "Then, if samples $x_1,\\ldots,x_N$ from $p(x)$ are available the above expectation can be approximated by its empirical average over the samples\n",
    "$$ \\mathbb{E}_p[f] \\approx \\frac{1}{N} \\sum_{i=1}^N f(x_i) .$$\n",
    "This approximation is known as *Monte-Carlo estimation*.\n",
    "\n",
    "Formally, Monte-Carlo estimation can be understood as integrating $f$ over the empirical measure of the samples:\n",
    "$$ P_{N} = \\frac{1}{N} \\sum_{i=1}^N \\delta_{x_i} $$\n",
    "where $\\delta_x(A) = \\mathbb{1}_A(x)$ denotes the Dirac measure. In this sense, the Monte-Carlo estimate is an exact expectation $\\mathbb{E}_{P_N}[f]$, but over the empirical measure $P_N$ instead of the actual measure with density $p(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is Monte-Carlo estimation so popular? Note that to solve the (possibly high-dimensional) integral $\\int f(x) p(x) dx$ the most important points are where the integrand $f(x) p(x)$ take high values. Monte-Carlo sampling, by drawing from the distribution $p$, automatically focuses on point with high probability $p(x)$.\n",
    "\n",
    "As any statistical procedure, the quality of the Monte-Carlo approximation $\\hat{\\mu} = \\frac{1}{N} \\sum_{i=1}^N f(x_i)$ for $\\mathbb{E}_p[f] = \\mu$ can be evaluated in terms of\n",
    "\n",
    "* **bias**, i.e. $\\mathbb{E}_p[\\hat{\\mu}] - \\mu = 0$\n",
    "* and **variance**, i.e. $\\mathbb{V}ar_p[\\hat{\\mu}] = \\mathbb{E}_p[(\\hat{\\mu} - \\mu)^2]$\n",
    "\n",
    "**Exercise**: Show that Monte-Carlo estimation is unbiased.\n",
    "\n",
    "To compute the variance, denote the variance of a single sample $x$ as $\\sigma^2 = \\mathbb{E}_p[ (f(x) - \\mu)^2 ] = \\mathbb{V}ar_p[f]$. Then, since samples are independent, the variance of $\\hat{\\mu}$ is found to be\n",
    "$$ \\mathbb{V}ar_p[\\frac{1}{N} \\sum_{i=1}^N f(x_i)] = \\frac{1}{N^2} \\sum_{i=1}^N \\mathbb{V}ar_p[f(x_i)] = \\frac{1}{N} \\sigma^2 $$\n",
    "Interestingly, this variance scales as $\\frac{1}{N}$ independent of the dimension of the space. Basically, this observation is the reason why Monte-Carlo integration is effective in high-dimensions.\n",
    "\n",
    "Note: Also probabilities can be expressed as expectations, i.e.\n",
    "$$ Pr(X \\in A) = \\int_A p(x) dx = \\int \\mathbb{1}_A(x) p(x) dx = \\mathbb{E}_p[\\mathbb{1}_A] $$\n",
    "Basically, this is the reason that the histogram of the samples can be used to vizualize their underlying probability density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard distribution and inversion sampling\n",
    "\n",
    "Most basic sampling methods assume that a source for uniform random numbers is available.\n",
    "\n",
    "Let $U$ denote a uniform random variable, i.e. $u \\in [0,1]$ and $P(U \\leq u) = u$. Then, the distribution of $X = f(U)$ has the probability distribution\n",
    "$$ P(X \\leq x) = P(f(U) \\leq x) = P(U \\leq f^{-1}(x)) = f^{-1}(x)$$\n",
    "assuming that $f$ is invertible.\n",
    "\n",
    "Now, suppose we want to sample from some distribution with distribution function $P(X \\leq x) = h(x)$. How do we have to choose $f$ such that $P(X \\leq x)$ has the desired form?\n",
    "\n",
    "From the above calculation, we see that $f = h^{-1}$ achieves the desired result. Thus, the following algorithm allows to draw samples from $X$ with distribution function $h(x)$:\n",
    "\n",
    "* Draw a uniform random number $u \\in [0,1]$\n",
    "* $x = h^{-1}(u)$ is then a sample from the desired distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next plot illustrates this method for the standard Gaussian distribution with density\n",
    "$$ p(x) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2} x^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def phi(x):\n",
    "    return norm.pdf(x)\n",
    "\n",
    "def Phi(x):\n",
    "    return norm.cdf(x)\n",
    "\n",
    "def Phi_inv(x):\n",
    "    return norm.ppf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-5, 5, 0.01)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(x, phi(x), 'g-')\n",
    "plt.plot(x, Phi(x), 'b-')\n",
    "plt.xlabel('x', fontsize=15)\n",
    "plt.ylabel('Probability', fontsize=15)\n",
    "plt.legend(['pdf', 'cdf'], fontsize=10)\n",
    "plt.title(\"Probability density function/ \\ncumulative distribution function\", fontsize=15);\n",
    "\n",
    "u = np.arange(0, 1, 0.01)\n",
    "plt.subplot(122)\n",
    "plt.plot(u, Phi_inv(u), 'b-')\n",
    "plt.xlabel('Probability', fontsize=15)\n",
    "plt.ylabel('x', fontsize=15)\n",
    "plt.title(\"Inverse cumulative distribution function\", fontsize=15);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we show the sampling algorithm in action, we define a general interface for sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "class Sampling (object):\n",
    "    \"\"\"\n",
    "    Abstract base class for all sampling methods.\n",
    "    \n",
    "    Subclasses need to implement self.sample()\n",
    "    \"\"\"\n",
    "    def sample(self):\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Default is to show class\n",
    "        \"\"\"\n",
    "        return str(self.__class__)\n",
    "    \n",
    "class InversionSampling (Sampling):\n",
    "    def __init__(self, h_inv):\n",
    "        self.h_inv = h_inv\n",
    "        \n",
    "    def sample(self):\n",
    "        return self.h_inv(np.random.uniform())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and an interactive plotter to show our sampler in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "import ipywidgets\n",
    "from ipywidgets import widgets\n",
    "from IPython import display\n",
    "\n",
    "def show_sampling(sampling, plotter, N=1000, exp=True,  \n",
    "                  f_exp=lambda x: np.power(x,2), true_exp=None, ):\n",
    "    '''Show the histogramm and the expectaton for the given sampling method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sampling(Sampling) : the given sampling method\n",
    "    plotter(func) : the plotting function\n",
    "    N(int) : the amount of samples\n",
    "    exp(bool) : if True the expectation will be shown\n",
    "    f_exp(func) : the function to calculate the expectation\n",
    "    true_exp(None or float) : the true mean value of the target distribution (if it is given)\n",
    "    '''\n",
    "    \n",
    "    def draw():        \n",
    "        return [sampling.sample() for _ in range(N)]\n",
    "    \n",
    "    def calculate_expectation(samples):\n",
    "        n = np.arange(1,N+1)\n",
    "        tmp = f_exp(samples)         \n",
    "        return(np.cumsum(tmp)/n)\n",
    "    \n",
    "    def show(redraw_check, num=100, bins=6): \n",
    "        # Check first, if we have to show the expectation\n",
    "        if exp == True:\n",
    "            plt.figure(figsize=(14, 5))\n",
    "            plt.subplot(121)\n",
    "            plotter(plt, samples[0][:num], bins)      \n",
    "            plt.subplot(122)\n",
    "            plt.plot(np.arange(1,N+1), calculate_expectation(samples), 'b-')        \n",
    "            plt.axvline(num, color='red', alpha=0.7)\n",
    "            if (true_exp is not None):\n",
    "                plt.axhline(true_exp,  color='green', alpha=0.5) \n",
    "            \n",
    "        else:  \n",
    "            plotter(plt, samples[0][:num], bins)        \n",
    "    \n",
    "    def click(b):\n",
    "        samples[0]=draw()\n",
    "        check.value = not (check.value)\n",
    "        \n",
    "    # Draw the samples \n",
    "    samples = [draw()]\n",
    "    \n",
    "    # Create and show widgets\n",
    "    check = widgets.Checkbox(value=False)\n",
    "    button = widgets.Button(description=\"Redraw\")        \n",
    "    button.on_click(click)\n",
    "    slider_samples = widgets.IntSlider(value=100, min=10, max=N, step=10, \n",
    "                                       description='Samples:', continuous_update=False)\n",
    "    slider_bins = widgets.IntSlider(value=7, min=1, max=30, step=1, \n",
    "                                    description='Bins:', continuous_update=False)\n",
    "    ui = widgets.HBox([button, slider_bins, slider_samples])        \n",
    "    out = widgets.interactive_output(show, {'redraw_check': check, 'num' : slider_samples, \n",
    "                                            'bins' : slider_bins})\n",
    "    display.display(ui, out)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a demonstration of inversion sampling to draw samples from a standard Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "def gauss_hist (ax, data, bins=6):\n",
    "    x = np.arange(-4,4,0.01)\n",
    "    ax.hist(data, bins, normed=True)\n",
    "    ax.plot(x, phi(x), 'r-', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampling = InversionSampling(Phi_inv)\n",
    "   \n",
    "show_sampling(sampling, plotter=gauss_hist, f_exp=lambda x: np.power(x,2), true_exp=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can illustrate that expectations computed on the samples converge to the true value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "def show_expectation (sampling, f):\n",
    "    \"\"\"\n",
    "    Show how the expectation of f converges when increasing the number of samples\n",
    "    \"\"\"\n",
    "    n = np.arange(1,2500)\n",
    "    tmp = [f(sampling.sample()) for _ in n]\n",
    "    \n",
    "    Ef = np.cumsum(tmp)/n\n",
    "    plt.plot(n, Ef, 'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_expectation(sampling, lambda x: x**2)\n",
    "plt.axhline(1.0,  color='r'); # The true variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "\n",
    "* Does your favorite library function, i.e. *numpy.random.normal*, use inversion sampling? If not, which algorithm is most used instead?\n",
    "* How does the Box-Muller method work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
